<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.0">Jekyll</generator><link href="https://hipml.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hipml.github.io/" rel="alternate" type="text/html" /><updated>2025-02-03T08:33:01-06:00</updated><id>https://hipml.github.io/feed.xml</id><title type="html">unamusings</title><subtitle>Unamusings. Thoughts and projects of a thoughtful tinkerer
</subtitle><author><name>Paul Lambert</name></author><entry><title type="html">Welcome to Jekyll!</title><link href="https://hipml.github.io/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2025-02-03T08:07:18-06:00</published><updated>2025-02-03T08:07:18-06:00</updated><id>https://hipml.github.io/welcome-to-jekyll</id><content type="html" xml:base="https://hipml.github.io/welcome-to-jekyll.html"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name>Paul Lambert</name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry><entry><title type="html">Variable Arrays In Slurm</title><link href="https://hipml.github.io/variable-arrays-in-slurm.html" rel="alternate" type="text/html" title="Variable Arrays In Slurm" /><published>2025-01-31T00:00:00-06:00</published><updated>2025-01-31T00:00:00-06:00</updated><id>https://hipml.github.io/variable-arrays-in-slurm</id><content type="html" xml:base="https://hipml.github.io/variable-arrays-in-slurm.html"><![CDATA[<p>Arrays are one of Slurm’s most powerful features for parallel job submission.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --array=1-100</span>

<span class="nv">input_file</span><span class="o">=</span><span class="s2">"data_</span><span class="k">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="k">}</span><span class="s2">.txt"</span>
<span class="nv">output_file</span><span class="o">=</span><span class="s2">"results_</span><span class="k">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="k">}</span><span class="s2">.txt"</span>

./my_program <span class="nv">$input_file</span> <span class="nv">$output_file</span></code></pre></figure>

<p>In general, Slurm is quite flexible, and that extends to arrays. There are a number of built in features:</p>

<ul>
  <li>Sequential range: <code class="language-plaintext highlighter-rouge">--array=1-100</code>,</li>
  <li>With step size: <code class="language-plaintext highlighter-rouge">--array=1-100:2</code> (odd numbers only),</li>
  <li>Specific values: <code class="language-plaintext highlighter-rouge">--array=1,3,5,7</code></li>
</ul>

<p>However, one feature that Slurm doesn’t have is <em>variable sized</em> arrays. What if we want a reusable HPC launch script to run over some number of simulations, but that number varies based on other inputs?</p>

<p>I found an elegant solution when working on my undergraduate thesis: using Slurm to launch another Slurm script.</p>

<p>This is a functional proof-of-concept Slurm script for initializing a variable array batch job:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --account=myaccount</span>
<span class="c">#SBATCH --partition=secondary</span>
<span class="c">#SBATCH --time=00:30:00</span>
<span class="c">#SBATCH --nodes=1</span>
<span class="c">#SBATCH --ntasks-per-node=1</span>
<span class="c">#SBATCH --job-name=rng_poc</span>
<span class="c">#SBATCH --output=logs/%x_%j_%a.out</span>
<span class="c">#SBATCH --error=logs/%x_%j_%a.err</span>

<span class="c"># Generate random size between 1-5</span>
<span class="nv">RANDOM_SIZE</span><span class="o">=</span><span class="k">$((</span>RANDOM <span class="o">%</span> <span class="m">5</span> <span class="o">+</span> <span class="m">1</span><span class="k">))</span>

<span class="c"># If this is the initial submission, create and submit new array job</span>
<span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c"># Create temporary script with same frontmatter</span>
    <span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">' &gt; tmp_array_job.sh
#!/bin/bash
#SBATCH --account=myaccount
#SBATCH --partition=secondary
#SBATCH --time=00:30:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --job-name=child_rng_poc
#SBATCH --output=logs/%x_%j_%a.out
#SBATCH --error=logs/%x_%j_%a.err

echo "This is array task </span><span class="k">${</span><span class="nv">SLURM_ARRAY_TASK_ID</span><span class="k">}</span><span class="sh">"
</span><span class="no">EOF

</span>    <span class="c"># Submit array job with random size</span>
    sbatch <span class="nt">--array</span><span class="o">=</span>0-<span class="k">$((</span>RANDOM_SIZE-1<span class="k">))</span> tmp_array_job.sh
    <span class="nb">rm </span>tmp_array_job.sh
    <span class="nb">exit </span>0
<span class="k">fi</span>

<span class="c"># Rest of your script for array tasks...</span></code></pre></figure>

<p>To wit: I wanted to prune a variable number of layers out of a given LLM, from 0 layers pruned to (n-1). Below is a sample script using this technique with an associative array to store the number of decoder layers per autoregressive model in our testing suite.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH --account=its-a-me</span>
<span class="c">#SBATCH --partition=low-requirement-box</span>
<span class="c">#SBATCH --time=00:30:00</span>
<span class="c">#SBATCH --nodes=1</span>
<span class="c">#SBATCH --ntasks-per-node=1</span>
<span class="c">#SBATCH --job-name=multiqlora</span>
<span class="c">#SBATCH --output=logs/%x/%j.out</span>
<span class="c">#SBATCH --error=logs/%x/%j.err</span>

<span class="c"># define model layers lookup</span>
<span class="nb">declare</span> <span class="nt">-A</span> <span class="nv">MODEL_LAYERS</span><span class="o">=(</span>
    <span class="o">[</span><span class="s2">"llama321b"</span><span class="o">]=</span>16
    <span class="o">[</span><span class="s2">"llama323b"</span><span class="o">]=</span>28
    <span class="o">[</span><span class="s2">"llama3170b"</span><span class="o">]=</span>80
    <span class="o">[</span><span class="s2">"llama38b"</span><span class="o">]=</span>32
    <span class="o">[</span><span class="s2">"qwen257b"</span><span class="o">]=</span>28
    <span class="o">[</span><span class="s2">"qwen2505b"</span><span class="o">]=</span>24
    <span class="o">[</span><span class="s2">"qwen2514b"</span><span class="o">]=</span>48
    <span class="o">[</span><span class="s2">"qwen2532b"</span><span class="o">]=</span>64
    <span class="o">[</span><span class="s2">"nemotron"</span><span class="o">]=</span>80
<span class="o">)</span>

<span class="c"># check if all arguments are provided</span>
<span class="k">if</span> <span class="o">[</span> <span class="nv">$# </span><span class="nt">-ne</span> 2 <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Usage: </span><span class="nv">$0</span><span class="s2"> &lt;model&gt; &lt;task&gt;"</span>
    <span class="nb">exit </span>1
<span class="k">fi

</span><span class="nv">MODEL_NAME</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
<span class="nv">TASK</span><span class="o">=</span><span class="s2">"</span><span class="nv">$2</span><span class="s2">"</span>

<span class="c"># get the number of layers for the model</span>
<span class="nv">NUM_LAYERS</span><span class="o">=</span><span class="k">${</span><span class="nv">MODEL_LAYERS</span><span class="p">[</span><span class="nv">$MODEL_NAME</span><span class="p">]</span><span class="k">}</span>
<span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$NUM_LAYERS</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Error: Unknown model '</span><span class="nv">$MODEL_NAME</span><span class="s2">'"</span>
    <span class="nb">echo</span> <span class="s2">"Available models: </span><span class="k">${</span><span class="p">!MODEL_LAYERS[@]</span><span class="k">}</span><span class="s2">"</span>
    <span class="nb">exit </span>1
<span class="k">fi

</span><span class="nb">echo</span> <span class="s2">"Model: </span><span class="nv">$MODEL_NAME</span><span class="s2">, layers </span><span class="nv">$NUM_LAYERS</span><span class="s2">"</span>

<span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">' &gt; tmp_array_job.sh
#!/bin/bash
#SBATCH --account=its-a-me
#SBATCH --partition=massive-gpu-cluster
#SBATCH --gres=gpu:4
#SBATCH --time=12:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=20
#SBATCH --mem=256G
#SBATCH --job-name=gmultitrain
#SBATCH --output=logs/%x/%a.out
#SBATCH --error=logs/%x/%a.err

# debug mode
# set -e
# set -x

# Load necessary modules and activate Conda environment
module load anaconda3/2024.06-Jun
module load cuda/12.4
module load texlive/2019
source /home/lamber10/.bashrc
export BNB_CUDA_VERSION=124
conda activate res_env
cd /projects/research/thesis/

mkdir -p logs

# Get command line arguments
MODEL_NAME="</span><span class="nv">$1</span><span class="sh">"
TASK="</span><span class="nv">$2</span><span class="sh">"

# run the evaluation script
torchrun --nproc_per_node=4 src/multitrain.py --model "</span><span class="nv">$MODEL_NAME</span><span class="sh">" --task "</span><span class="nv">$TASK</span><span class="sh">" --nltp "</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="sh">"
</span><span class="no">EOF

</span>    <span class="c"># Submit array job</span>
    sbatch <span class="nt">--array</span><span class="o">=</span>0-<span class="k">$((</span>NUM_LAYERS-1<span class="k">))</span> tmp_array_job.sh <span class="s2">"</span><span class="nv">$MODEL_NAME</span><span class="s2">"</span> <span class="s2">"</span><span class="nv">$TASK</span><span class="s2">"</span>
    <span class="nb">rm </span>tmp_array_job.sh
    <span class="nb">exit </span>0
<span class="k">fi</span>
</code></pre></div></div>

<p>While it might seem a bit like Inception-style job scheduling, it cleanly solves the problem of variable-sized arrays without requiring external dependencies or complex bash wizardry. The parent job stays lightweight - it just needs to determine the array size and hand off the actual computation to its children.</p>

<p>In my case, this let me write reusable training scripts that could handle any model architecture without modifications. But the pattern works just as well for other variable-sized workloads, from dataset preprocessing to hyperparameter sweeps. Next time you find yourself hard-coding array sizes or maintaining multiple versions of the same script, consider letting Slurm launch Slurm instead.</p>]]></content><author><name>Paul Lambert</name></author><category term="coding" /><summary type="html"><![CDATA[Arrays are one of Slurm’s most powerful features for parallel job submission.]]></summary></entry></feed>